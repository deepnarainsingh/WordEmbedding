{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Embedding for a given corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Using T8 corpus available from Word2Vec in gensim to create word embeddings\n",
    "## Before training Word2Vec doing pre-processing on the data \n",
    "## Using nltk tokenizer to tokenize the text from file into sentence and further sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_txt2_sentences(text):\n",
    "    \"\"\"\n",
    "     Split text into sentences \n",
    "     Takes input as text\n",
    "     return string with each line as input from text\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # split text into sentences using nltk packages\n",
    "    for sentence in tokenizer.tokenize(txt):\n",
    "        yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_txt2_words(text, lower=True, remove_stop_words=True):\n",
    "    \"\"\"\n",
    "    Split text into words list\n",
    "    Takes input as text\n",
    "    Text converted in lowercase\n",
    "    Remove the stop words\n",
    "    return list of words from input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    if lower:\n",
    "        txt = txt.lower()\n",
    "    words = SentenceWord2Vec.RE_WIHTE_SPACES.split(txt.strip().lower())\n",
    "    if remove_stop_words:\n",
    "        #remove stop words from text\n",
    "        words = [w for w in words if w not in SentenceWord2Vec.STOP_WORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SentenceWord2Vec(object):\n",
    "    \"\"\"\n",
    "    This class returns Sentences from input text files directory\n",
    "    \"\"\"\n",
    "    RE_WIHTE_SPACES = re.compile(\"\\s+\")\n",
    "    STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "   \n",
    "    def __init__(self, dirname):\n",
    "        \n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Iterate through directory and return\n",
    "        sentences as list of words\n",
    "        \"\"\"\n",
    "        #Iterating on the input directory\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            # read line from file (Without reading the entire file)\n",
    "            for line in file(os.path.join(self.dirname, fname), \"rb\"):\n",
    "                line=unicode(line,\"utf-8\")\n",
    "                # split the read line into sentences using NLTK\n",
    "                for s in conv_txt2_sentences(line):\n",
    "                    # split the sentence into words using regex\n",
    "                    word =conv_txt2_words(s, lower=True, remove_stop_words=False)\n",
    "                    \n",
    "                    yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Directory to read the corpus for training Word2Vec\n",
    "PATH_DIR = \"data\"\n",
    "sentences = SentenceWord2Vec(PATH_DIR+\"/txt\")\n",
    "sentences.dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training Word2Vec\n",
    "model = Word2Vec(sentences, size=300, workers=4, min_count=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tried training this corpus using Word2Vec and it was taking lot of time so had to stop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Once trained model is there we have the word embeddings from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Intent Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "- Assuming we have the labelled data available for intent classification.\n",
    "\n",
    "- Use the Word2Vec model to map each word into its corresponding vector.\n",
    "\n",
    "- Calculate the average vector of all the word vectors.\n",
    "\n",
    "- We will use the average calculated vector as input to our classfication algorithms.\n",
    "\n",
    "- For base model we can use classification algorithm like Logistic Regression,SVM , Naive Bayes etc and compare the results.\n",
    "\n",
    "- For advanced model we can use multilayer perceptron or use RNN and LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some pseudo code from keras for MLP model for multiclass classification.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, input_dim=20, init='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, init='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, init='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=20,\n",
    "          batch_size=16)\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
